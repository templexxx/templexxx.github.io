<html>
    <head>

        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <link href="http://www.templex.xyz/main.css" rel="stylesheet" type="text/css">
        <link rel="shortcut icon" href="http://www.templex.xyz/images/3x3.jpg">

        <title>Silent Data Corruption 幽灵杀手</title>

    </head>

    <body>
        <div style="margin: 2% 5% 2% 5%">
            <table width="80%">
                <tr>
                    <td width="60%">
                        <div style="padding: 2% 8% 5% 8%; border: 1px solid LightGrey;">
                            <h2>Silent Data Corruption 幽灵杀手</h2>
                            <p>为了提高数据在存储系统中的可靠性，我们使用冗余的办法来抵御不稳定因素，如副本或纠删码。
                                然而单凭冗余策略，我们不仅不能完全保障数据安全，可能还会面临系统雪崩的风险。这类被大多数人忽视，
                                或是不甚了解的潜在错误无时无刻不在威胁着我们的系统安全。</p>
                            <p>那么是什么如幽灵一般存在呢？我们如何抗击它呢？在这篇文章中，我们将一起来追踪这位幽灵杀手。</p>
                            <h3>Silent Data Corruption，原来是你</h3>
                            <p>通常我们认为硬件是完全可靠的，然而，有些错误可以穿透这些校验机制，从而造成潜在的危害与事故。
                                我们称这一类错误为 Silent Data Corruption (SDC)。尽管 SDC 出现的概率并不高，但对于有大量数据交互的服务来说，
                                可以认为它是必现的，有时它能造成难以修复的故障。</p>
                            <h3>“亡魂”</h3>
                            <ol>
                            <li>AWS S3 因 bit corruption 造成 36 小时的服务中断[1]</li>
                            <li>Facebook 遇到 10%-15% 的图片无法访问 (48 小时)[2]</li>
                            <li>Netflix 宕机超过 3 天[3]</li>
                            <li>...</li>
                            </ol>
                            <p>以上这些重大事故的无一例外，都是 Silent Data Corruption 这位鬼魅般的数据杀手的 “杰作”。</p>
                            <img src="../../images/sdc/boo.png" width="80%">
                            <h3>捉鬼有方</h3>
                            <p>我们固然可以执行非常严厉的计划来避免 SDC 问题，然而无论多么精巧的设计都不能 100% 避免问题的发生，而且会急剧降低系统的整体性能。这就要求我们学会在 性能 与 可靠性 之间做工程上的平衡。</p>
                            <p>但这并不意味着放弃抵抗或是拍脑袋决策，我们率先需要认真考察 SDC 出现的场景和影响。从而进行提出有针对性的方案。</p>
                            <h4>对数据流分类</h4>
                            <h5>对数据流分类</h5>
                            <p>根据数据的目的，我们可以将数据流分为 控制流 和 文件流 两种形态。这两种形态对于数据的完整性提出了不同的要求。对于控制流，我们必须保证整条数据流中任意两端的完整性，这是因为控制流出错可能引发出大的灾难。</p>
                            <p>对于文件流，我们只需要提供最终完整的机制即可。如客户可以拿本地的 SHA1 与存储系统返回的 SHA1 进行对比。（值得注意的是，任意一端的 SHA1 可能并不反映真实的数据情况）</p>
                            <h5>分类依据：媒介</h5>
                            <p>根据数据的媒介，我们可以将数据流分为 网络流 和 本地流 两种形态。分别代表服务器之间的通信过程和服务内部的通信过程。</p>
                            <h3>幽灵魅影</h3>
                            <p>在本小节点中，我们将根据上文的分类去分析 SDC 出现的场景以及预防机制。</p>
                            <h4>网络流</h4>
                            <p>在应用层之下，我们已经拥有 switch 的 CRC32 与 TCP Checksum 的防护机制，然而数据经流的每一台 switch 都会重新计算 CRC32 (除非是两台机器简单的通过一台 switch 相连)，也就是说，目前我们拥有的机制无法避免 switch 内部错误。如果经过了路由，情况会变得更加复杂。 TCP Checksum 的设计目标之一就是为了预防这样的错误，但其强度太低，不足以胜任这样的任务。[4]</p>
                            <img src="../../images/sdc/switch.png" width="80%">
                            <p>CRC32 与 Adler32 (Fletcher 的替代品) 的设计目的都是为了做以太网 frame (around 1500B) 的校验，其中 Adler32 的强度还要低于 Fletcher32[5]</p>
                            <p>我们所需要考察的是，在较大的数据尺度上（校验在应用层上做），这两种算法能否满足我们提高系统可靠性的要求。</p>

                            <h5>出错原因</h5>
                            <p>主要为 bit flipping 错误，以及交换机/路由的固件和逻辑问题。因此既要预防少量的 bit 随机反转，也要能应对成片的错误。在交换机日志上可能会看到被交换机纠错过的记录，如果错误日志相关条目的数据量非常大，可以考虑进行对交换机的维护/更换。</p>

                            <h5>所需要校验的数据特征</h5>
                            <p>上文中，我们指出应对数据流做区别对待，其中要更加注重控制流（即指令参数等控制信息）。原因如下：</p>
                            <ol>
                                <li>对于存储系统来说，我们并不能控制客户端行为。如果客户端在上传时不提供数据摘要，也就是说不能实现完整的 end-to-end data integrity</li>
                                <li>文件流危害范围小，不会造成系统雪崩</li>
                            </ol>
                            <p>控制流呈现如下特点：</p>
                            <ol>
                                <li>数据量相对较小，通常小于 1MB。哪怕设计到集群信息也一般在百MB以下</li>
                                <li>信息相对比较重要，部分出错需要零容忍。比如文件唯一 id 由于 bit flipping 出现重复</li>
                                <li>数据一般进行了封装，以便 unmarshal 成可以使用的数据结构。而数据结构本身具备一定的抗随机错误的能力</li>
                            </ol>

                            <h5>校验算法</h5>
                            <p>无论是 CRC32 IEEE802.3 还是 Adler32 或者 Fletcher32 在上述最大数据尺度上其 hamming distance 均为 2。对于校验算法来说，hamming distance 指的是其最少多少位错误的情况下会有 1bit 无法检测。这里的 2 是指，只要有 2bit 错误至少会有 1bit 错误无法被识别到。</p>
                            <p>因此，如果觉得强度不满意。还可以选择非加密的散列算法，如 xxhash[6] 或 seahash[7]。</p>

                            <h4>本地流</h4>
                            <p>与磁盘进行数据交互的可以分为三类：定位，读取，写入这三个过程互相组合，需要我们有不同的应对方案</p>
                            <h5>定位</h5>
                            <p>主要是写定位错误 (Misdirected writes) ，数据写入的磁盘位置与预期不符。这个错误可能导致两个文件同时被污染。</p>

                            <h5>读取</h5>
                            <p>最常见的 SDC 错误发生过程，这是因为其本质上可能是多种错误导致的最终结果。最常见的情况有：</p>
                            <ol>
                                <li>超过扇区 ECC 检错能力的错误</li>
                                <li>读取过程中的 bit flipping</li>
                                <li>所读取的数据在写入过程中的错误</li>
                            </ol>

                            <h5>写入</h5>
                            <p>主要为写丢失 (lost writes)，这个错误意味这没有写入成功的情况下返回写入完成（注意和掉电导致的缓存数据未刷入非易失性存储介质的情况相区别）。其后果是在正确的位置读到旧数据。</p>
                            <p>另外，需要注意的是，SDC 在 HDD 上有扩散趋势[8]</p>

                            <h5>磁盘保护</h5>
                            <p>磁盘厂商针对 SDC 能做的是尽量保证数据在磁盘内不出问题，各家厂商都有自己的 end-to-end integrity 技术。我们的存储系统主要是由 SATA 和少量 SSD 组成。服务器系统盘则可能为 SAS 盘（以上均为企业级磁盘）。其中绝大多数的 SATA 盘中并没有 SDC 防御机制。</p>
                            <p>SAS 盘提供 Fat-Sector 来进行保护。其中包括 CRC, 地址标签，程序标签。[9]</p>
                            <p>SSD 的 flash media 层以及 DRAM 都不易发生 bit flipping 。但在 controller 中有大量 SRAM CMOS 是非常容易发生错误的。 Intel 为企业级 SSD 在做了一系列保护之后，还进行了 high intensity particle beams 实验来做 SDC 测试[10]</p>

                            <h5>文件系统保护</h5>
                            <p>xfs 提供了元数据的一致性保护[11]，其措施能避免 读 与 misdirected 错误。由于对象存储不存在修改文件元数据信息的场景，因此 lost writes 的未处理可以接受。相对而言， zfs 则提供了全面的一致性校验[12]。但 zfs 的文件级别的保护并不适用于对象存储 append-only 的场景。</p>

                            <h5>Scrub</h5>
                            <p>Scrub 可以分为两类，一类是 Media Scrubs (可以通过 SCSI cmd 触发)，另一种是 Data Scrubs。这分别代表磁盘内部对扇区进行扫描，以及在应用层对数据摘要的对比扫描。</p>
                            <p>我们常说的就是 Data Scrubs。这种 Scrub 的目的是检测 silent corruption，它可能说明本地流中某个或某几个环节有问题，但不能直接表明扇区有错误，尽管我们可以通过这样一个过程来发现扇区故障。</p>

                            <h3>实践</h3>
                            <h4>控制流</h4>
                            <p>必须进行 end-to-end integrity 校验。使用 seahash 算法在请求体中追加校验值，接受方必须进行数据对比。</p>
                            <h4>副本集群</h4>
                            <p>副本集群采用最终一致性的策略要预防 SDC。对于多个副本来说，同时发生写错误的概率微乎其微。因此我们在读时校验，在校验失败后锁定相应磁盘即可。</p>
                            <p>需要注意的是，将原始数据与摘要分开存放并不能处理 misdirected writes 错误。</p>
                            <p>对于我们所使用的 xfs 文件系统来说，由于其开启了 metadata 保护，因此文件系统层面很难出现 misdirected 错误。因此，倘若我们要进行 misdirected writes 保护，只需要在原先的 checksum 前加上该数据在大文件块中的偏移量即可。偏移量一致且能返回数据和 checksum 的概率几乎可以忽略不计（在数据错误的情况下)</p>
                            <img src="../../images/sdc/checksum.png" width="80%">
                            <p>特别需要说明的是 lost writes 错误，lost writes 的应对方案主要有两种：</p>
                            <ol>
                                <li>read-after-write</li>
                                <li>checksum 与 data 分开存放</li>
                            </ol>
                            <p>这两种方案的代价都非常高昂。在对象存储 append-only 的场景下，lost writes 发生且校验一致的概率微乎其微。因此 lost writes 在副本集群下不采取特别的措施。</p>
                            <h4>纠删码集群</h4>\
                            <p>EC 由于没有副本机制保护，因此必须进行 misdirected writes 保护，其方法与多副本中描述一致，这里就不在赘述了</p>
                            <h4>数据库集群</h4>
                            <p>数据库要面临的 SDC 风险与本地流一致，利好消息是数据库集群为多副本架构。在 chekcusm match 之后，就意味着获得的值不会造成系统风险。如果发生了 misdirected writes 或者 lost writes 错误导致查询失败，我们几乎可以肯定的能从其它副本中获得正确的数值。除了加入 checksum 字段之外，为数据库所使用的 SSD 必须符合我们对于可靠性的要求。</p>


                            <h3>结束语</h3>
                            <p>此篇文章如图幽灵一般结束了。。。</p>

                            <h3>附录</h3>
                            <ol>
                                <li>Amazon S3 Avaliability Event: July 20, 2008, http://status.aws.amazon.com/s3-20080720.html</li>
                                <li>Facebook temporarily loses more than 10% of photos in hard drive failure, http://www.computerworld.com/s/article/9129263</li>
                                <li>Netflix Outage Blamed on Hardware, https://www.pcmag.com/article2/0,2817,2328778,00.asp</li>
                                <li>Jonathan Stone and Craig Partridge, When The CRC and TCP Checksum Disagree</li>
                                <li>Theresa Maxino, Revisiting Fletcher and Adler Checksums</li>
                                <li>xxhash, http://www.xxhash.com</li>
                                <li>SeaHash: Explained: http://ticki.github.io/blog/seahash-explained/</li>
                                <li>An Analysis of Data Corruption in the Storage Stack</li>
                                <li>HGST End-to-end Data Protection</li>
                                <li>Data integrity in solid state drives: what supernovas mean to you, https://itpeernetwork.intel.com/data-integrity-in-solid-statedrives-what-supernovas-mean-to-you/</li>
                                <li>XFS Self Describing metadata, https://www.kernel.org/doc/Documentation/filesystems/xfs-self-describing-metadata.txt</li>
                                <li>End-to-end Data Integrity for File Systems: A ZFS Case Study</li>
                            </ol>





                            <p>以RS 码为例，纠删码实现于具体的存储系统可以分为几个部分：编码、解码和修复过程中的计算都是在有限域上进行的；编码过程即是计算生成矩阵（范德蒙德或柯西矩阵）和所有数据的乘积；解码则是计算解码矩阵（生成矩阵中某些行向量组成的方阵的逆矩阵）和重建数据的乘积。</p>
                            <h4>有限域运算</h4>
                            <p>有限域是纠删码中运算的基础域，所有的编解码和重建运算都是基于某个有限域的。不止是纠删码，一般的编码方法都在有限域上进行，比如常见的AES 加密中也有有限域运算。使用有限域的一个重要原因是计算机并不能精确执行无限域的运算，比如有理数域和虚数域。</p>
                            <p>此外，在有限域上运算另一个重要的好处是运算后的结果大小在一定范围内，这是因为有限域的封闭性决定的，这也为程序设计提供了便利。比如在RS中，我们通常使用GF(2^8)，即0～255这一有限域，这是因为其长度刚好为1字节，便于我们对数据进行存储和计算。</p>
                            <p>在确定了有限域的大小之后，通过有限域上的生成多项式可以找到该域上的生成元[1]，进而通过生成元的幂次遍历有限域上的元素，利用这一性质我们可以生成相应的指数表。通过指数表我们可以求出对数表，再利用指数表与对数表最终生成乘法表。关于本原多项式的生成以及相关运算表的计算可以参考我在开源库中的数学工具。[2]</p>
                            <p>有了乘法表，我们就可以在运算过程中直接查表获得结果，而不用进行复杂的多项式运算了。同时也不难发现，查表优化将会成为接下来工作的重点与难点。</p>
                            <h4>选择生成矩阵</h4>
                            <p>生成矩阵(GM, generator matrix) 定义了如何将原始数据块编码为冗余数据块，RS 码的生成矩阵是一个 n 行 k 列矩阵，将 k 块原始数据块编码为 n 块冗余数据块。如果对应的编码是系统码(比如 RAID)，编码后包含了原始数据，则生成矩阵中包含一个 k×k 大小的单位矩阵和(n−k)×k 的冗余矩阵， 单位矩阵对应的是原始数据块，冗余矩阵对应的是冗余数据块。非系统码没有单位矩阵，整个生成矩阵都是冗余矩阵，因此编码后只有冗余数据块。通常我们会使用系统码以提高数据提取时的效率，那么接下来我们需要找到合适的冗余矩阵</p>
                            <p>在解码过程中我们要对矩阵求逆，因此所采用的矩阵必须满足子矩阵可逆的性质。目前业界应用最多的两种矩阵是Vandermonde matrix (范德蒙矩阵）和Cauchy matrix（柯西矩阵）。其中范德蒙矩阵历史最为悠久，但需要注意的是我们并不能直接使用范德蒙矩阵作为生成矩阵，而需要通过高斯消元后才能使用，这是因为在编码参数（k+m)比较大时会存在矩阵不可逆的风险。</p>
                            <p>柯西矩阵运算简单，只不过需要计算乘法逆元，我们可以提前计算好乘法逆元表以供生成编码矩阵时使用。创建以柯西矩阵为生成矩阵的编码矩阵的伪代码如下图所示：</p>
                            <img src="../../images/reedsolomon/code1.jpg" width="80%">
                            <h4>矩阵求逆运算</h4>
                            <p>有限域上的求逆方法和我们学习的线性代数中求逆方法相同，常见的是高斯消元法，算法复杂度是O(n^3)。过程如下：</p>
                            <ol>
                            <li>在待求逆的矩阵右边拼接一个单位矩阵</li>
                            <li>进行高斯消元运算</li>
                            <li>取得到的矩阵左边非单位矩阵的部分作为求逆的结果，如果不可逆则报错</li>
                            </ol>
                            <p>我们在实际的测试环境中发现，矩阵求逆的开销还是比较大的(大约 6000 ns/op)。考虑到在实际系统中，单盘数据重建往往需要几个小时或者更长（磁盘I/O 占据绝大部分时间），求逆计算时间可以忽略不计。</p>
                            <h3>进一步的工程优化</h3>
                            <h4>利用SIMD加速有限域运算</h4>
                            <p>从上一篇文章可知，有限域上的乘法是通过查表得到的，每个字节和生成矩阵中元素的乘法结果通过查表得到，图1 给出了按字节对原始数据进行编码的过程（生成多项式为 x^8 + x^4 + x^3 + x^2 + 1）。对于任意 1 字节来说，在GF(2^8)内有256种可能的值，所以没有元素对应的乘法表大小为 256 字节。每次查表可以进行一个字节数据的乘法运算，效率很低。</p>
                            <img src="../../images/reedsolomon/table.png" width="80%">
                            <p>目前主流的支持SIMD相关指令的寄存器有128bit（XMM 指令)、256bit(YMM 指令)这两种容量，这意味着对于64位的机器来说，分别提供了2到4倍的处理能力，我们可以考虑采用SIMD 指令并行地为更多数据进行乘法运算。</p>
                            <p>但每个元素的乘法表的大小为 256 Byte ,这大大超出了寄存器容纳能力。为了达到利用并行查表的目的，我们采用分治的思想将两个字节的乘法运算进行拆分。</p>
                            <p>字节 y 与字节 a 的乘法运算过程可表示为，其中y(a)表示从 y 的乘法表中查询与 x 相乘结果的操作：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>y(a) = y * a</code></pre>
                            </div>
                            <p>我们将字节 a 拆分成高4位（al) 与低 4 位 (ar) 两个部分，即（其中 ⊕ 为异或运算）：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>a = (al << 4) ⊕ ar</code></pre>
                            </div>
                            <p>这样字节 a 就表示为 0-15 与 (0-15 << 4) 异或运算的结果了。</p>
                            <p>于是原先的 y 与 a 的乘法运算可表示为：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>y(a) = y(al << 4) ⊕ y(ar)</code></pre>
                            </div>
                            <p>由于ar 与 al 的范围均为 0-15（0-1111），字节 y 与它们相乘的结果也就只有16个可能的值了 。这样原先256 字节的字节 y 的乘法表就可以被 2 张 16 字节的乘法表替换了。</p>
                            <p>下面以根据本原多项式 x^8 + x^4 + x^3 + x^2 + 1 生成的 GF(2^8) 为例，分别通过查询普通乘法表与使用拆分乘法表来演示 16 * 100 的计算过程。</p>
                            <p>16 的完整乘法表为：</p>
                            <img src="../../images/reedsolomon/ftable.jpg" width="80%">
                            <p> 计算 16 * 100 可以直接查表得到：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>table[100] = 14</code></pre>
                            </div>
                            <p>16 的低4位乘法表，也就是16 与 0-15 的乘法结果：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>lowtable = [0 16 32 48 64 80 96 112 128 144 160 176 192 208 224 240]</code></pre>
                            </div>
                            <p>16 的高4位乘法表，为16 与 0-15 << 4 的乘法结果：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>hightable = [0 29 58 39 116 105 78 83 232 245 210 207 156 129 166 187]</code></pre>
                            </div>
                            <p>将 100 （01100100）拆分，则</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>100 = 0110 << 4 ⊕ 0100</code></pre>
                            </div>
                            <p>在低位表中查询 0100（4），得：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>lowtable[4] = 64</code></pre>
                            </div>
                            <p>在高位表中查询0110 （6），得：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>hightable[6] = 78</code></pre>
                            </div>
                            <p>将两个查询结果异或：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>result = 64 ^ 78 = 1000000 ^ 1001110 = 1110 = 14</code></pre>
                            </div>
                            <p>从上面的对比中，我们不难发现采用SIMD的新算法提高查表速度主要表现在两个方面：</p>
                            <ol>
                            <li>减少了乘法表大小</li>
                            <li>提高查表并行度（从1个字节到16甚至32个字节）</li>
                            </ol>
                            <p>采用SIMD 指令在大大降低了乘法表的规模的同时多了一次查表操作以及异或运算。由于新的乘法表每一部分只有 16字节，我们可以顺利的将其放置于XMM寄存器中，从而利用SIMD指令集提供的指令来进行数据向量运算，将原先的逐字节查表改进为并行的对16字节进行查表，同时异或操作也是 16 字节并行的。除此之外，由于乘法表的总体规模的下降，在编码过程中的缓存污染也被大大减轻了，关于缓存的问题我们会在接下来的小节中进行更细致的分析。</p>
                            <p>以上的计算过程以单个字节作为例子，下面我们一同来分析利用SIMD技术对多个字节进行运算的过程。基本步骤如下：</p>
                            <ol>
                            <li>拆分保存原始数据的XMM寄存器中的数据向量，分别存储于不同的XMM寄存器中</li>
                            <li>根据拆分后的数据向量对乘法表进行重排，即得到查表结果。我们可以将乘法表理解为按顺序排放的数组，数组长度为16，查表的过程可以理解为将拆分后的数据（数据范围为 0-15）作为索引对乘法表数组进行重新排序。这样我们就可以通过排序指令完成查表操作了</li>
                            <li>将重排后的结果进行异或，得到最终的运算结果</li>
                            </ol>
                            <p>以下是伪代码：</p>
                            <img src="http://www.templex.xyz/images/reedsolomon/code2.jpg" width="80%">
                            <p>需要注意的是，要使用SIMD加速有限域运算，对CPU的最低要求是支持SSSE3扩展指令集。另外为了充分提高效率，我们应该事先对数据进行内存对齐操作，在SSSE3下我们需要将数据对齐到16 Bytes，否则我们只能使用非对齐指令进行数据的读取和写入。在这一点上比较特殊的是Go 语言， 一方面Go支持直接调用汇编函数这为使用SIMD指令集提供了语言上的支持；但另外一方面Golang 又隐藏了内存申请的细节，这使得指定内存对齐操作不可控，虽然我们也可以通过cgo或者汇编来实现，但这增加额外的负担。所幸，对于CPU来说一个Cache line 的大小为64byte，这在一定程度上可以帮助我们减少非对齐读写带来的惩罚。另外，根据Golang 的内存对齐算法，对于较大的数据块，Golang是会自动对齐到32byte的，因此对齐或非对齐指令的执行效果是一致的。</p>
                            <h3>写缓存友好代码</h3>
                            <p>缓存优化通过两方面进行，其一是减少缓存污染；其二是提高缓存命中率。在尝试做到这两点之前，我们先来分析缓存的基本工作原理。</p>
                            <p>CPU 缓存的默认工作模式是Write-Back, 即每一次读写内存数据都需要先写入缓存。上文提到的Cache line即为缓存工作的基本单位，其大小为固定的 64byte ，也就说哪怕从内存中读取 1字节的数据，CPU也会将其余的63 字节带入缓存。这样设计的原因主要是为了提高缓存的时间局域性，因为所要执行的数据大小通常远远超过这个数字，提前将数据读取至缓存有利于接下来的数据在缓存中被命中。</p>
                            <h4>矩阵运算分块</h4>
                            <p>矩阵运算的循环迭代中都用到了行与列，因此原始数据矩阵与编码矩阵的访问总有一方是非连续的，通过简单的循环交换并不能改善运算的空间局域性。因此我们通过分块的方法来提高时间局域性来减少缓存缺失。</p>
                            <p>分块算法不是对一个数组的整行或整列进行操作，而是对其子矩阵进行操作，目的是在缓存中的数据被替换之前，最大限度的利用它。</p>
                            <p>分块的尺寸不宜过大，太大的分块无法被装进缓存；另外也不能过小，太小的分块导致外部逻辑的调用次数大大上升，产生了不必要的函数调用开销，而且也不能充分利用缓存空间。</p>
                            <h4>减少缓存污染</h4>
                            <p>不难发现的是，编码矩阵中的系数并不会完全覆盖整个GF(2^8)，例如10+4 的编码方案中，编码矩阵中校验矩阵大小为4×10，编码系数至多（可能会有重复）有10×4=40 个。因此我们可以事先进行一个乘法表初始化的过程，比如生成一个新的二维数组来存储编码系数的乘法表。缩小表的范围可以在读取表的过程中对缓存的污染。</p>
                            <p>另外在定义方法集时需要注意的是避免结构体中的元素浪费。避免将不必要的参数扔进结构体中，如果每一个方法仅使用其中若干个元素，则其他元素白白侵占了缓存空间。</p>
                            <h3>指令级并行与数据级并行的深入优化</h3>
                            <p>本节主要介绍如何利用AVX/AVX2 指令集以及指令级并行优化来进一步提高性能表现。除此之外，我们还可以对汇编代码进行微调以取得微小的提升。比如，尽量避免使用 R8-R15 这8个寄存器，因为指令解码会比其他通用寄存器多一个字节。但很多汇编优化细节是和CPU架构设计相关的，书本上甚至Intel提供的手册也并不能提供最准确的指导（因为有滞后性），而且这些操作带来的效益并不显著，在这里就不做重点说明了。</p>
                            <h4>利用AVX2</h4>
                            <p>在上文中我们已经知道如何将乘法表拆分成128bits 的大小以适应XMM寄存器，那么对于AVX指令集来说，要充分发挥其作用，需要将乘法表复制到256bit的YMM寄存器。为了做到这一点，我们可以利用XMM 寄存器为YMM 寄存器的低位这一特性，仅使用一条指令来完成表的复制（Intel 风格）：</p>
                            <div class="highlighter-rouge"><pre class="highlight"><code>vinserti128 ymm0, ymm0, xmm0, 1</code></pre>
                            </div>
                            <p>这条指令作用是将xmm0寄存器中的数据拷贝到ymm0中,而剩余128位数据通过ymm0得到，其中立即数 1 表明xmm0拷贝的目的地是 ymm0 的高位。这条指令提供了两个source operand（源操作数）以及一个destination operand（目标操作数），我们在这里使用ymm0寄存器同时作为源操作数和目标操作数来实现了表的复制操作。接下来我们便可以使用与SSSE3下同样的方式来进行单指令32byte的编码运算过程了。</p>
                            <p>由于使用了SSE与AVX这两种扩展指令集，我们需要避免 AVX-SSE Transition Penalties[3]。之所以会有这种性能惩罚主要是由于SSE指令对YMM寄存器的高位一无所知，SSE指令与AVX指令的混用会导致机器不断的执行YMM寄存器的高位保存与恢复，这大大影响了性能表现。如果对指令不熟悉，难以避免指令混用，那么可以在RET前使用VZEROUPPER指令来清空YMM寄存器的高位。</p>
                            <h4>指令级并行 (ILP) 优化</h4>
                            <p>程序分支指令的开销并不仅仅为指令执行所需要的周期，因为它们可能影响前端流水线和内部缓存的内容。我们可以通过如下技巧来减少分支指令对性能的影响，并且提高分支预测单元的准确性：</p>
                            <ol>
                            <li>尽量少的使用分支指令</li>
                            <li>当贯穿(fall-through)更可能被执行时，使用向前条件跳转</li>
                            <li>当贯穿代码不太可能被执行时，使用向后条件跳转</li>
                            </ol>
                            <p>向前跳转经常用在检查函数参数的代码块中，如果我们避免了传入长度为0 的数据切片，这样可以在汇编中去掉相关的分支判断。在我的代码中仅有一条向后条件跳转指令，用在循环代码块的底部。需要注意的是，以上2 ， 3点中的优化方法是为了符合静态分支预测算法的要求，然而在市场上基于硬件动态预测方法等处理器占主导地位，因此这两点优化可能并不会起到提高分支预测准确度的作用，更多的是良好的编程习惯的问题。</p>
                            <p>对于CPU 的执行引擎来说，其往往包含多个执行单元实例，这是执行引擎并发执行多个微操做的基本原理。另外CPU内核的调度器下会挂有多个端口，这意味着每个周期调度器可以给执行引擎分发多个微操作。因此我们可以利用循环展开来提高指令级并行的可能性。</p>
                            <p>循环展开就是将循环体复制多次，同时调整循环的终止代码。由于它减少了分支判断的次数，因此可以将来自不同迭代的指令放在一起调度。</p>
                            <p>当然，如果循环展开知识简单地进行指令复制，最后使用的都是同一组寄存器，可能会妨碍对循环的有效调度。因此我们应当合理分配寄存器的使用。另外，如果循环规模较大，会导致指令缓存的缺失率上升。Intel的优化手册中指出，循环体不应当超过500条指令。[4]</p>
                            <h3>小结</h3>
                            <p>以上内容较为完整的还原了纠删码引擎的实现过程，涉及到了较多的数学和硬件层面的知识，对于大部分工程师来说可能相对陌生，我们希望通过本系列文章的介绍能够为大家的工程实践提供些许帮助。但受限于篇幅，很多内容无法全面展开。比如，部分数学工具的理论与证明并没有得到详细的解释，还需要读者通过其他专业资料的来进行更深入的学习。</p>
                            <h3>附录</h3>
                            <ol>
                            <li><a href="http://user.xmission.com/~rimrock/Documents/Galois%20Fields%20and%20Cyclic%20Codes.pdf">Galois Fields and Cyclic Codes </a></li>
                            <li><a href="https://github.com/templexxx/reedsolomon/tree/master/mathtools">有限域相关计算 </a></li>
                            <li><a href="https://software.intel.com/en-us/articles/avoiding-avx-sse-transition-penalties">Avoiding AVX-SSE Transition Penalties</a></li>
                            <li>Intel 64 and IA-32 Architectures Optimization Reference Manual :3.4.2.6 Optimization for Decoded ICache</li>
                            </ol>
                        </div>

                    </td>
                </tr>
            </table>
        </div>
    </body>

</html>
