<html>
    <head>
        <meta http-equiv="content-type" content="text/html; charset=utf-8" />
        <link href="http://www.templex.xyz/main.css" rel="stylesheet" type="text/css">
        <link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Atom feed">
        <link rel="shortcut icon" href="http://www.templex.xyz/images/3x3.jpg">
        <title>比拷贝内存还快的Reed-Solomon code</title>
</head>
<body>
    <h2>ReedSolomon</h2>
    <h3>为什么要造轮子？</h3>
    <p>Reed-Solomon codes已经问世50余年了，是门非常成熟的并且应用广泛的技术。</p>
    <p>在优化方面，公认的最佳做法就是利用SIMD技术加速(具体的方法我随后展开)。看上去其并不适宜重新造轮子，而且新轮子极大可能远远不如老轮子，但我觉得我有机会写出非常快的程序。</p>
    <p>我的初衷是通过这个过程学习一下群，域，矩阵等数学知识以及它们在计算机上的应用。在阅读一些著名的开源库的时候，我发现有一些疑问很多人都有，但
        大家似乎都没有很好的回答过，似乎这些问题非常的高深。这让我觉得很不应该，我本能的认为这些问题应该没那么难。于是在解决这些问题的过程中，我写完了这个库。</p>
    <p>最后的原因和我个人相关，作为一名销售转技术的非专业人士，在这个慢慢转变的过程中吃到了很多苦头。比如碰到一些关注度非常低的小问题，要找到满意的
    解答几乎是不可能的。由于各种机缘巧合我对RS code产生了浓厚的兴趣，那么借这个机会我想减轻对此感兴趣的其他人的负担和痛苦，这也算是传播一点微小的能量。</p>
    <h3>站在巨人的肩膀上</h3>
    <p>最早接触的是Jerasure的实现以及一些科普性的文章，那个时候连hello world都不会写，只留下了粗浅的印象。</p>
    <p>后来逐渐发现了更多的开源库,其中:<a href="https://github.com/klauspost/reedsolomon">Klauspost</a>参考<a href="https://github.com/Backblaze/JavaReedSolomon/">Backblaze</a>而实现。
    而我又参考着klauspost的实现。</p>
    <p>我主要参考了其汇编实现，然而原先的代码受限于go早期版本支持的指令有限，因此代码中很多机器码(虽然我也使用了机器码)。另外注释有些不清晰，
        我加上了一些关于指令执行过程的说明，这样对于使用者来说更友好。</p>
    <h3>工作原理</h3>
    <p>我这里并不试图展开细节，一方面事无巨细的介绍容易没有终点，二是因为涉及的知识点着实太多，每一门知识又有其必要的基础知识，这要求
    必须根据读者的水平阐述，在一篇文章中要做到这点是非常困难的。</p>
    <p>同时我又觉得我不能跳过数学原理的介绍。网络上的相关介绍要么太过简略，要么是一门非常系统的课程，
        总之少有人去引导整个过程的学习。所谓“授人以鱼不如授人以渔”，我便分享一下我的学习过程吧。</p>
    <p>首先，我们得对群这个概念和性质有基本的认识，这里我推荐台湾国立交通大学的化学应用群论这门公开课。其中有两节课是纯粹讲群的和化学无关，我觉得老师
    很有耐心，每一步的步骤非常清楚。这个时候我们已经能手工写一些order非常小的域了，我建议是自己列一列加法表和乘法表以增强印象。</p>
    <p>接下来可以阅读<<代数编码与密码>>这本书关于多项式的理论，学习如何通过多项式构建有限域。可以手算结果，然后上<a href="http://www.ee.unb.ca/cgi-bin/tervo/calc2.pl">UNB</a>
    验证结果。另外在我的代码<a href="https://github.com/templexxx/reedsolomon/blob/master/tools/gentables.go">gentables.go</a>中也可以学习生成本原多项式以及生成表的过程。</p>
    <p>我们还需要知道矩阵运算的基本常识，基本上掌握矩阵的乘法，矩阵求逆就足以应付了。我在这里推荐<<程序员的数学:线性代数>>这一书，尤其需要注意的
    知识点是Gauss-Jordan方法，这一方法也是目前代码中用来求逆矩阵的方法（由于我们使用的矩阵比较特殊，个人觉得求逆还可以再优化）</p>
    <p>有了矩阵的基础，我们对选择编码矩阵也有了基本的认识。我在代码中使用的是Cauchy matrix,生成的算法参考了Intel的ISA-L库。我们不能直接
    使用Vandermonde matrix，具体的原因我有在互联网世界的某一个角落的中提到。不过我认为如果掌握了上面的知识，大家也可以得到这个结论。</p>
    <p>数学的部分基本讲完了，还有一点数学知识是和优化相关的。其实非常的简单而优雅，寥寥几句就可以阐明核心,
    但我还是推荐大家读一下论文<a href="http://www.kaymgee.com/Kevin_Greenan/Publications_files/plank-fast2013.pdf">Screaming Fast Galois Field Arithmetic Using Intel SIMD Instructions</a></p>
    <p>其加速的原理就是讲一个byte以4bit为界限拆分成左右两个部分，每一个部分的乘法表只有16byte的长度，正好可以塞进XMM寄存器里。对于YMM寄存器，我们则可以塞两张表进去。</p>
    <p>利用CPU提供的指令，以数据为MASK，对表进行重新排序，然后异或左右两个部分的值我们就得到了编码后的结果了。</p>
    <h3>代码分析</h3>
    <p>代码量很少，除去测试和表，总计500行不到。再抛开上层的业务逻辑，核心部分只有分片并发和使用SIMD技术的汇编代码。</p>
    <p>我们暂且先不考虑计算部分，那么很显然的，耗时主要在内存读写上，而矩阵运算是需要反复读取数据的，基于以上两点，得出了一个很清晰的优化目标：写缓存友好代码。</p>
    <p>有了这样的思路，我直觉地使用了L1 DATA Cache Size作为了我的并发单位，果不其然获得了非常好的性能体验。具体的原因下文会提到。</p>
    <p>另外，控制并发数也是非常有必要的。有几个物理核心就有几套寄存器和L1 Cache，因此盲目加大并发既是一种浪费又造成了不必要的竞争（虽然通过设置逻辑核心
        与物理核心是可以共享L1的，但对于我们的并发模型来说没有用处），并发数限定在物理核心数即可。</p>
    <p>我之前有提过Golang的汇编代码是相对友好的，它抽象了出了一些汇编的细节，我们可以不用过分纠结于寄存器的高低位，堆栈的维护，参数的读取。当然一般的汇编代码陷阱依然是存在的。</p>
    <p>很容易犯的一个错误是使用了“脏”寄存器的数据，这主要是寄存器重用错误和高低位处理上的疏忽。Golang的汇编帮我们减少了第二种错误的头脑负担，不过XMM,YMM寄存器的高低位问题还是要我们自己小心的。</p>
    <p>高低位问题带来的不仅仅是烦恼，我们同样可以利用它写出优雅的汇编代码。比如在<a href="https://github.com/templexxx/reedsolomon/blob/master/gf_amd64.s">gf_amd64.s</a>中读取“左右表”
    到YMM寄存器的代码就处理的比较精彩，充分利用了X0作为Y0的低位这一性质。</p>
    <p>为了提高汇编代码的吞吐性能，常见的处理手段是Non-Temporal Hint或者prefetch,然而这两种技术我们都用不到。我们的数据不存在无时态问题，使用non-temporal hint技术去写入内存反倒会大大降低性能表现。
    另外prefetch更多是对CPU的提示，CPU可以选择忽略。倘若是非常小的数据需要反复使用，那么使用预取会有一定的性能提升，但我们的使用场景下，总体需要读取的数据大大超过了缓存的尺寸，加之在合适的并发分片下，
    缓存工作的非常好，预取在这里就没有必要使用了。</p>
    <p>我这里还需要提一下的是数据对齐的问题，我的代码中并没有使用这样的技巧，因为我发现数据对齐并没有提高性能,这是个反直觉的结果。
        实际上，较新的Intel处理器现在都不用费太多心思去使用内存对其的MOV指令了。</p>
    <p>当汇编代码中同时出现XMM,YMM寄存器时，我们需要多一个心眼。道理是非常简单的，对于SSE指令来说，它是“看不见”YMM寄存器的高位的，在执行过程中，只好将YMM的
    高位先保存下来以备后续使用。而同样是处理128bit的寄存器，AVX指令则没有这个问题，因为它知道高位的存在，在指令执行中顺手把高位清零了。</p>
    <p>在我的汇编代码中，我非常小心的去除了SSE指令的痕迹，想办法使用了AVX指令做了替换，这样我就避免AVX-SSE Transition Penalties</p>
    <p>这里顺带分享一下我寻找所需要的指令的经验，一条条去遍历手册固然是可以，但对于我这种脑容量有限的人来说不一会就迷失于各种细节之中了。因此保持
        对指令的“大局观”就格外重要了。我们得首先对CPU会提供什么样的指令有粗略的感官体验，然后抱着自己的目的去翻查。哪怕这个世界上并没有你想要的
        指令，事后也可以通过这个过程摸点门道出来。</p>
    <p>这里我以SIMD技术为例，对于CPU少得可怜的功能来说，SIMD的适用范围更窄，无非就是数据传输，算术运算，比较，转换，逻辑操作，位移，
    排序，插入等。说得再简单一点就是知道从哪里拿数据，简单操作一下，找个地方放回去这三个步骤。对于一项我们想要的功能，分这三步分别找到合适的指令。
    当发现可以用时，写个简单的测试程序跑跑看，如果找到了更多的类似指令，分别运行对比一下。用不了很长时间，最合适的那一条就出炉了。</p>
    <p>我这里以“将掩码移动到YMM”为例重现这个过程：</p>
    <ol>
    <li>首先分析功能，掩码0x0f长度1字节，因此我们要找到一条能复制byte到YMM的指令。很显然这是一条VEX指令，直观上来说就是指令的名字是"V"打头的，
    不一会我们就会找到”VPBROADCASTB“”这条指令。</li>
    <li>通过阅读这条指令的文档，我们知道0x0f要么来自内存，要么来自XMM寄存器。对于CPU来说，读取速度最快的就是通过寄存器了，其次是立即数，最次是内存，在我们这种情况下，
    自然是想办法将0x0f移动到XMM</li>
    <li>那么现在我们需要一条能将一字节移动到XMM的指令，我们自然想到的是mov操作，但是mov操作的源和地址基本是等长的关系，看来还得另找办法。</li>
    <li>暂时找不到思路，我们就打开指令手册，根据目录一条条过滤吧。因为有了目标还是能比较快的发现“VPINSRB”这条指令的。这里大家可能会疑惑为什么不用
    看上去更简单的“PINSRB”指令，这是为了避免上面提到的AVX-SSE Transition Penalties</li>
    <li>接下来就是将0x0f移动到一个8bit的寄存器上，我这里就不展开了。</li>
    </ol>
    <p>我在汇编代码中加了比较详细的注释，更多的细节大家可以通过代码以及注释去了解。</p>
    <p>实际上，代码总共的内容就只有这么点————一些数学工具，汇编的encode引擎。</p>
    <h3>Performance</h3>
    <p>如果要一句话总结性能表现的话，我想说这是一个比内存拷贝还要快的RS引擎。</p>
    <p>在我的电脑上将一个16MB的[]byte用golang内置的copy函数，拷贝到另一个切片上的速度达到了7GB/S(我用Novabench跑出的RAM Speed是7544MB/S),而我对28个16MB的块计算一份冗余的速度超过了16GB/S，
        听上去有些不可思议，但它的能量就是这么超乎想象。</p>
    <p>无论编码还是解码，其性能都是非常强劲的，我优化了解码的逻辑，去除了额外的内存拷贝，使得解码的速度和编码一样快。在我同事的笔记本(i7-6700HQ)上对256KB大小的数据块做10x4的encode速度超过了20GB/S</p>
    <p>知名的性能优秀的库在编码上（也就是库中汇编代码的工作）的优化思路基本都是一样的，其区别主要是在并发控制，以及并发的块大小</p>
    <p>关于通过并发尺寸优化性能的问题，
        我在上面卖了个关子，我这里提一下其原因。这是因为在计算parity时，需要对原有parity反复读取，倘若这一部分数据进了L1 Cache，那自然是惊人的快了。但是单
        凭这一点还是不足以跑赢内存拷贝。</p>
    <p>我们回顾一下冗余块的计算过程，我们会发现过程上整个系统的读和写应该是一个量级的，但最终只会有若干份（冗余块的数量）被真正写到内存中。奥秘就在这里了。</p>
    <p>对于一次计算循环来说，理论上冗余数据一直在L1中被命中，而且读取的下一份数据均与上一步的写入的内存地址不相同。这里要感谢伟大的乱序执行技术，对于这样的情景
        乱序执行会将率先执行下一步的读操作，也就是说最终真正被执行写入内存的数据只有其中一小份。</p>
    <p>加之内存读的速度是快于写的（实际上，MOV指令从内存中复制数据到寄存器要比从寄存器中复制数据到内存快上不少），这样我们单在IO上就至少跑赢内存拷贝一倍多了。
    省下的指令无非就是读两张表，位移一下数据，然后排排序，是耗不了多少时间的。</p>
    <img src="http://templex.xyz/images/reedsolomon/cpuCacheRAM.jpg" width="80%">
    <p>上面这张图大致反映了数据流动的情况，问题的核心也被表现的非常突出：即缓存命中率，缓存读取速度以及写优化。</p>
    <h3>代码</h3>
    <p>代码在这里：<a href="https://github.com/templexxx/reedsolomon">templexxx</a></p>
    </body>
    </html>
